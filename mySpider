import urllib.request
from bs4 import BeautifulSoup
from urllib.request import urlretrieve
import os
import codecs
import pymongo
#建立MongoDB的连接，插入数据时会自动创建指定的数据库及集合
myclient = pymongo.MongoClient("mongodb://localhost:27017/")
mydb = myclient["spider"]
mycol = mydb["article"]
#指定爬取的文件导出路径
filePath = r"c:/spider/"
#判断路径是否存在，不存在则创建路径
if not os.path.exists(filePath):
    os.makedirs(filePath)
#个人网站根路径
url = "http://wysh.site"
#article_list 存放网站文章的url，初始为空
article_list = []
#设置一个用户代理，模拟浏览器
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
#通过 urllib 发送 http 请求
req = urllib.request.Request(url, headers={
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
})

# 打开网页链接，针对请求发送过程中可能产生的异常
# （比如网络不稳定导致请求发送失败）进行异常补捕获并重试，最多重试10次
def openlink_and_retry(req):
    maNum = 10
    for tries in range(maNum):
        try:
            response = urllib.request.urlopen(req)
            return response
        except:
            if tries < (maNum):
                print("retry!!!")
                continue
            else:
                print("Has tried %d times , all failed!", maNum)
                break
# 请求网站首页的页面数据
print('发送首页的页面网络请求: ' + url)
response = openlink_and_retry(req)
content = response.read().decode('utf-8')
#使用 BeautifulSoup 对响应数据进行解析
soup = BeautifulSoup(content, "lxml")
spans = soup.select('span.space')[0]
# 获取页面数量
total_page = spans.nextSibling.get_text()
print("total_page is :" + total_page)
#除首页外，其他页面的公共部分
urlBase = "http://wysh.site/page/"
#页面索引
index = 1
total_page = int(total_page)
#依次遍历对网站的每一个分页进行遍历，获取分页中所有文章的 url
while index <= total_page:
    # 索引大于 1 的时候需要重新指定 url
    if index > 1:
        # 拼装成每页的 url
        url = urlBase + str(index)
        req = urllib.request.Request(url, headers={
            'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
        })
        print('发送页面网络请求 : ' + url)
    response = openlink_and_retry(req)
    content = response.read().decode('utf-8')
    soup = BeautifulSoup(content, "lxml")
    articles = soup.select(".post-block")
    #遍历分页中的每一篇文章
    for article in articles:
        # 从页面中提取出每一篇文章的 url
        article_url = article.select("link")[0].attrs['href']
        print(article_url)
        # 将文章的 url 添加到文章列表中
        article_list.append(article_url)
    # 每循环一次后，索引加一，指向下一页
    index = index + 1
j = 1
# 依次从文章列表中取出每一篇文章的 url ，通过BeautifulSoup 提取该文章的相关信息
for i in article_list:
    print(i)
    # url中有中文，因此我们需要使用 urllib.request.quote(link)
    # 来把链接中的中文编码成 url 中的正确编码
    trans_url = urllib.request.quote(i)
    # 这里会把: 也转码了，转换成 % 3
    # 因此，转换之后，我们还需要还原为:
    urlMain = trans_url.replace('%3A', ':')
    print(urlMain)
    req = urllib.request.Request(urlMain, headers={
        'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
    })
    response = openlink_and_retry(req)
    content = response.read().decode('utf-8')
    soup = BeautifulSoup(content, "lxml")
    # 获取文章的标题
    heading = soup.h1.string
    heading1 = "标题是 ：" + heading
    rep = soup.select("#posts > article > div > header > div > span.post-time > time")[0].get_text()
    #去掉首位空格、回车、换行
    rep = rep.strip().replace("\n", "").replace("\r", "")
    # 获取发布时间
    rep1 = "\n" + "发表于：" + rep
    print(heading1)
    print(rep1)
    print(str(j))
    # 获取文章的分类标签列表，一篇文章可能属于多个分类
    cata_tag = soup.find_all("span", itemprop="about", itemscope="", itemtype="http://schema.org/Thing")
    #标签名称列表
    cata_list = []
    for c in cata_tag:
        # 将标签名称添加到标签名称列表里面
        cata_list.append(c.text.strip().replace("\n", "").replace("\r", ""))
    #将标签名称列表转化为字符串
    cata_name = "，".join(cata_list)
    cata_name1 = "\n" + "分类于： " + cata_name
    print(cata_name1)
    # 获取文章字数
    word_count = soup.find("span", title="字数统计").text.strip().replace("\n", "").replace("\r", "")
    word_count1 = "\n" + "字数是： " + word_count
    print(word_count)
    j = j + 1
    #获取文章正文
    body = soup.select("#posts > article > div > div.post-body.han-init-context")[0]
    #获取正文最前面的引文
    blockquote = body.blockquote
    #获取引文信息
    if blockquote != None:
        quote = blockquote.text.strip()
        quote1 = "\n" + "引文是 ： " + quote
    else:
        quote = None
        quote1 = "\n" + "引文为空"
    print(quote1)
    #正文的内容初始值为空
    content = ''
    # 对正文的孩子节点（直接子节点）进行遍历，获取正文内容
    for nextTag in body.children:
        #如果正文的子节点有 text 属性，则获取其文本
        if (hasattr(nextTag, 'text')):
            tagContent = nextTag.text.strip()
        else:
            tagContent = "没有 text 属性 ！！！"
            continue
        print(tagContent)
        #如果正文的子节点是一号标题
        if nextTag.name == 'h1':
            tagContent = "一号标题： " + tagContent + '\n'
            content = content + tagContent
            # file.write("一号标题： " + tagContent + '\n')
            continue
        # 如果正文的子节点是二号标题
        if nextTag.name == 'h2':
            tagContent = "二号标题： " + tagContent + '\n'
            content = content + tagContent
            # file.write("二号标题：" + tagContent + '\n')
            continue
        # 如果正文的子节点是三号标题
        if nextTag.name == 'h3':
            tagContent = "三号标题： " + tagContent + '\n'
            content = content + tagContent
            # file.write("三号标题：" + tagContent + '\n')
            continue
        # 如果正文的子节点是四号标题
        if nextTag.name == 'h4':
            tagContent = "四号标题： " + tagContent + '\n'
            content = content + tagContent
            # file.write("四号标题：" + tagContent + '\n')
            continue
        # 如果正文的子节点是代码块
        if nextTag.select('figure').__len__() > 0 or nextTag.name == 'figure':
            # 如果 select 的 length 大于 0 则表示这个元素包含 figure 的元素
            if nextTag.select('figure').__len__() > 0:
                nextTag = nextTag.select('figure')[0]
            #代码内容，初始值为空
            codeLine = ''
            #依次读取并添加每一行代码内容
            for line in nextTag.table.tr.find('td', attrs={'class': 'code'}).find_all('span', attrs={'class': 'line'}):
                codeLine += line.text.strip() + '\n'
            codeLine  = "代码块： "+ "\n" + codeLine
            content = content + codeLine
            print(codeLine)
            # file.write("代码块： "+ codeLine )
            continue
        # 如果正文的子节点是无序列表
        if nextTag.name == 'ul':
            #列表内容初始值为空
            li_text = ''
            for li in nextTag.find_all('li'):
                li_text = li_text + '- ' + li.text.strip() + '\n'
                # file.write('- ' + li.text.strip() + '\n')
            content = content + li_text
            continue
        # 如果正文的子节点是有序列表
        if nextTag.name == 'ol':
            olIndex = 1
            li_text = ''
            for li in nextTag.find_all('li'):
                li_text = li_text + str(olIndex) + '. ' + li.text.strip() + '\n'
                # file.write(str(olIndex) + '. ' + li.text.strip() + '\n')
                olIndex += 1
            content = content + li_text
            continue
        #如果正文的子节点是段落
        if nextTag.name == 'p':
            tagContent = nextTag.text.strip()
            # 为空表示是图片
            if tagContent == '':
                #获取图片的 url
                pic_url = nextTag.find('img')['src']
                content =  content + pic_url
                # file.write(pic_url)
                #获取图片的名称
                pic_name = nextTag.find('img')['alt']
                urlretrieve(pic_url, filePath + heading + '__' + pic_name)
                continue
            else:
                links = nextTag.find_all('a')
                for link in links:
                    tagContent = tagContent.replace(link.text, link['href'])
                content = content + tagContent
                # file.write(tagContent + '\n')
                continue
    i1 = '文章的 url 是 ： ' + i
    file = codecs.open(filePath + '/' + heading + '.txt', "w", encoding='utf8')  # 指定文件的编码格式
    #将文章的标题、url 、发布时间、字数、引文、正文信息写入文件
    file.writelines(heading1)
    file.writelines(i1)
    file.writelines(rep1)
    file.writelines(cata_name1)
    file.writelines(word_count1)
    file.writelines(quote1)
    file.writelines(content)
    # 使用urlretrieve下载文章中的图片到本地
    file.close()
    post = {"heading":heading,"url":i,"reportTime":rep,"category":cata_name,"wordCount":word_count,"quote":quote,"content":content}
    try:
        mycol.insert_one(post)
    except:
        print("写入 MongoDB 出错 ！！！")
    finally:
        #关闭 MongoDB 连接
        myclient.close()
